model:
  sequence_length: 8
  embedding:
    vocab_size: 128
  hidden_size: 64
  layers: 2
  attention:
    heads: 4
    rotary_dim: 8
  eval: True
  precision: "float32"
training:
  global_batch_size: 2
execution:
  micro_batch_size: 2
  data_parallel: 1
  attention_serialisation: 2
